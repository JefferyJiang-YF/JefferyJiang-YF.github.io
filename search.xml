<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2019/07/02/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>hexo</category>
        <category>theme</category>
      </categories>
      <tags>
        <tag>hello</tag>
        <tag>world</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇文章</title>
    <url>/2024/07/07/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<h1 id="Hexo-博客-新建文章"><a href="#Hexo-博客-新建文章" class="headerlink" title="Hexo 博客 新建文章"></a>Hexo 博客 新建文章</h1><p>输入如下命令，创建一篇新的文章</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>layout 是文章的布局，默认为post，可以先不写。</p>
</li>
<li><p>title 是文章的标题，也是文件的名字，存储在source&#x2F;_posts下。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost blog]#hexo new &quot;我的第一篇文章&quot;</span><br><span class="line">INFO  Validating config</span><br><span class="line">INFO  Created: D:\blog_hexo\myblog\source\_posts\我的第一篇文章.md</span><br></pre></td></tr></table></figure>
<p>可以看到创建的文件在<code>\source\_posts</code>路径下，之后在MarkDown编辑器上打开就可以编辑你的文章内容。<br>打开创建的MarkDown文件，默认内容如下，我们可以在新建的文件里用MarkDown语法编辑文章</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 我的第一篇文章</span><br><span class="line">date: 2022-03-15 17:57:05</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<h1 id="布局"><a href="#布局" class="headerlink" title="布局"></a>布局</h1><p>Hexo 有三种默认布局：<code>post</code>、<code>page</code> 和 <code>draft</code>在创建者三种不同类型的文件时，它们将会被保存到不同的路径。</p>
<ul>
<li>post<ul>
<li>默认布局，用于文章。</li>
<li>保存在<code>source/_posts</code>目录下。</li>
</ul>
</li>
<li>page<ul>
<li>用于单页。</li>
<li>保存在<code>source</code>目录下。</li>
</ul>
</li>
<li>draft<ul>
<li>用于草稿。</li>
<li>保存在<code>source/_drafts</code>目录下。<br>如果你不想你的文章被处理，你可以将 Front-Matter 中的layout: 设为 false 。</li>
</ul>
</li>
</ul>
<p>Front-matter 参数<br>Front-matter 是文章最上方以—分隔的区域，用于指定文章的变量设置<br><img data-src="https://tuchuang.org.cn/imgs/2024/07/06/9dd63d3ad75489a1.png"></p>
<h1 id="NexT主题-多层级分类"><a href="#NexT主题-多层级分类" class="headerlink" title="NexT主题 多层级分类"></a>NexT主题 多层级分类</h1><p>1.单层分类<br>如果你的一篇文章只分一类，可以像如下方式设置分类名：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- Hexo 博客</span><br></pre></td></tr></table></figure>
<p>2.父子分类<br>如果你想让一篇文章处于父类中的子类中，可以像如下方式设置分类名：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- 前端</span><br><span class="line">- 笔记</span><br></pre></td></tr></table></figure>
<p>3.并列分类<br>如果你想让一篇文章处于同一层的不同类中，可以像如下方式设置分类名：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [后端]</span><br><span class="line">- [笔记]</span><br></pre></td></tr></table></figure>
<p>4.同一父类不同子类<br>如果你想让一篇文章处于同一父类的不同类中，可以像如下方式设置分类名：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [学习,html]</span><br><span class="line">- [学习,http]</span><br></pre></td></tr></table></figure>
<p>总体效果图：<br><img data-src="https://tuchuang.org.cn/imgs/2024/07/06/9b053c5df042c6bf.png"></p>
]]></content>
  </entry>
  <entry>
    <title>Statistical Information in NLP</title>
    <url>/2024/07/09/Statistical-Information-in-NLP/</url>
    <content><![CDATA[<p>TF-IDF（Term Frequency-Inverse Document Frequency）是一种常用的文本特征提取方法，用于评估一个词在一个文档中的重要性。除了标准的TF-IDF，还有一些变体和扩展来进一步优化特征提取过程。以下是一些常见的变体：</p>
<ol>
<li><p><strong>BM25</strong>：</p>
<ul>
<li><p>BM25（Best Matching 25）是TF-IDF的一种改进版本，广泛用于信息检索。它考虑了词频饱和现象和文档长度规范化，使得对长文档和短文档的处理更加平衡。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{BM25}(q, D) &#x3D; \sum_{i&#x3D;1}^{n} IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}<br> $$</p>
<p> 其中，f(q_i, D)是词q_i在文档D中的频率，|D|是文档长度，\text{avgdl}是平均文档长度，k_1和b是调节参数。
 </p>
</li>
<li><p><strong>TF-CHI</strong>：</p>
<ul>
<li><p>这种方法结合了词频（TF）和卡方检验（Chi-Square Test，CHI），在考虑词频的同时，引入卡方统计量来衡量词与类别之间的相关性。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{TF-CHI}(t, c) &#x3D; \text{TF}(t, c) \cdot \chi^2(t, c)<br> $$</p>
<p> 其中，\chi^2(t, c)是词t和类别c之间的卡方统计量。
 </p>
</li>
<li><p><strong>TF-IG</strong>：</p>
<ul>
<li><p>结合了词频和信息增益（Information Gain, IG），通过信息增益来评估词对分类的贡献。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{TF-IG}(t, c) &#x3D; \text{TF}(t, c) \cdot \text{IG}(t, c)<br> $$</p>
<p> 其中，\text{IG}(t, c)是词t和类别c之间的信息增益。
 </p>
</li>
<li><p><strong>TF-RF</strong>：</p>
<ul>
<li><p>结合了词频和相关系数（Relevance Frequency, RF），用于评估词与类别之间的相关性。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{TF-RF}(t, c) &#x3D; \text{TF}(t, c) \cdot \text{RF}(t, c)<br> $$</p>
<p> 其中，\text{RF}(t, c)是词t在类别c中的相关系数。
 </p>
</li>
<li><p><strong>LDA-TF-IDF</strong>：</p>
<ul>
<li><p>将主题模型（LDA, Latent Dirichlet Allocation）与TF-IDF结合，通过LDA生成主题分布后，再基于这些主题分布计算TF-IDF。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{LDA-TF-IDF}(t, D) &#x3D; \text{TF-IDF}(t, D) \cdot P(z | D)<br> $$</p>
<p> 其中，P(z | D)是文档D中主题z的概率。
 </p>
</li>
<li><p>**Okapi BM25+**：</p>
<ul>
<li><p>BM25的进一步改进版本，加入了一些附加参数来增强模型性能。</p>
</li>
<li><p>公式类似于BM25，但加入了文档饱和度和文档权重的参数。</p>
</li>
</ul>
</li>
<li><p><strong>Weighted TF-IDF</strong>：</p>
<ul>
<li><p>在计算TF-IDF时，对词频或逆文档频率进行加权。例如，可以根据词的词性、词的重要性、领域专有词等因素进行加权。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{Weighted TF-IDF}(t, d) &#x3D; w_t \cdot \left( \frac{\text{TF}(t, d)}{\text{DF}(t)} \right)<br> $$</p>
<p> 其中，w_t是词t的权重，可以根据不同的标准来设定。
 </p>
</li>
<li><p><strong>Logarithmic TF-IDF</strong>：</p>
<ul>
<li><p>使用对数变换来平滑词频，减小高频词对模型的影响。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{Log-TF}(t, d) &#x3D; \log(1 + \text{TF}(t, d))<br> $$</p>
<p> $$<br> \text{Log-TF-IDF}(t, d) &#x3D; \text{Log-TF}(t, d) \cdot \text{IDF}(t)<br> $$</p>
</li>
<li><p><strong>Sublinear TF-IDF</strong>：</p>
<ul>
<li><p>对词频进行子线性缩放，通常用于大规模文档集。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br> \text{Sublinear-TF}(t, d) &#x3D; 1 + \log(\text{TF}(t, d))<br> $$</p>
<p> $$<br> \text{Sublinear-TF-IDF}(t, d) &#x3D; \text{Sublinear-TF}(t, d) \cdot \text{IDF}(t)<br> $$</p>
</li>
<li><p><strong>Double Normalization TF-IDF</strong>：</p>
</li>
</ol>
<ul>
<li><p>通过双重归一化对词频进行标准化，常用于处理文档长度差异。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br>  \text{DoubleNorm-TF}(t, d) &#x3D; 0.5 + 0.5 \cdot \frac{\text{TF}(t, d)}{\max_{t’} \text{TF}(t’, d)}<br> $$<br> $$<br>  \text{DoubleNorm-TF-IDF}(t, d) &#x3D; \text{DoubleNorm-TF}(t, d) \cdot \text{IDF}(t)<br> $$</p>
<ol start="11">
<li><strong>TF-IDF with Class-Based Weighting</strong>：</li>
</ol>
<ul>
<li><p>根据词在不同类别中的分布情况进行加权，以增强分类任务的效果。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br>  \text{Class-TF-IDF}(t, d, c) &#x3D; \text{TF-IDF}(t, d) \cdot \text{ClassWeight}(t, c)<br> $$<br>  其中，$\text{ClassWeight}(t, c)$表示词$t$在类别$c$中的重要性权重。</p>
<ol start="12">
<li><strong>TF-IDF with Query Expansion</strong>：</li>
</ol>
<ul>
<li><p>在查询扩展过程中使用TF-IDF，对初始查询进行扩展，以包含更多相关的词。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br>  \text{Expanded-TF-IDF}(q, d) &#x3D; \sum_{t \in q \cup E(q)} \text{TF-IDF}(t, d)<br> $$<br>  其中，$E(q)$是查询$q$的扩展词集合。</p>
<ol start="13">
<li>**Smooth Inverse Frequency (SIF)**：</li>
</ol>
<ul>
<li><p>用于词向量加权，通过平滑逆频率来减少高频词的影响。</p>
</li>
<li><p>公式：</p>
</li>
</ul>
<p> $$<br>  \text{SIF}(t) &#x3D; \frac{a}{a + \text{TF}(t)}<br> $$<br>  其中，$a$是一个小的平滑参数。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><p><strong>BM25</strong>：</p>
<ul>
<li>Robertson, S. E., Walker, S., Beaulieu, M. M., Gatford, M., &amp; Payne, A. (1996). Okapi at TREC-4. In D. K. Harman (Ed.), <em>NIST Special Publication 500-236: Proceedings of The Fourth Text REtrieval Conference (TREC-4)</em> (pp. 73-96). Gaithersburg, MD: National Institute of Standards and Technology. Retrieved from <a href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_921">SpringerLink</a>.</li>
</ul>
</li>
<li><p><strong>TF-CHI</strong>：</p>
<ul>
<li>Yang, Y., &amp; Pedersen, J. O. (1997). A comparative study on feature selection in text categorization. In <em>Proceedings of the Fourteenth International Conference on Machine Learning (ICML)</em> (pp. 412-420). San Francisco, CA: Morgan Kaufmann.</li>
</ul>
</li>
<li><p><strong>TF-IG</strong>：</p>
<ul>
<li>Yang, Y., &amp; Pedersen, J. O. (1997). A comparative study on feature selection in text categorization. In <em>Proceedings of the Fourteenth International Conference on Machine Learning (ICML)</em> (pp. 412-420). San Francisco, CA: Morgan Kaufmann.</li>
</ul>
</li>
<li><p><strong>TF-RF</strong>：</p>
<ul>
<li>Lan, M., Tan, C. L., Su, J., &amp; Lu, Y. (2009). Supervised and traditional term weighting methods for automatic text categorization. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 31</em>(4), 721-735. <a href="https://doi.org/10.1109/TPAMI.2008.110">https://doi.org/10.1109/TPAMI.2008.110</a></li>
</ul>
</li>
<li><p><strong>LDA-TF-IDF</strong>：</p>
<ul>
<li>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent Dirichlet Allocation. <em>Journal of Machine Learning Research, 3</em>, 993-1022. Retrieved from <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">JMLR</a>.</li>
</ul>
</li>
<li><p><strong>Okapi BM25+</strong> Robertson, S., Zaragoza, H., &amp; Taylor, M. (2004). Simple BM25 extension to multiple weighted fields. In <em>Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management</em> (pp. 42-49). ACM. <a href="https://doi.org/10.1145/1031171.1031181">https://doi.org/10.1145/1031171.1031181</a></p>
</li>
<li><p><strong>Weighted TF-IDF</strong>：</p>
<ul>
<li>Baeza-Yates, R., &amp; Ribeiro-Neto, B. (1999). <em>Modern Information Retrieval</em>. Addison-Wesley Longman Publishing Co., Inc.</li>
</ul>
</li>
<li><p><strong>Logarithmic TF-IDF</strong>：</p>
<ul>
<li>Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). <em>Introduction to Information Retrieval</em>. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511809071">https://doi.org/10.1017/CBO9780511809071</a></li>
</ul>
</li>
<li><p><strong>Sublinear TF-IDF</strong>：</p>
<ul>
<li>Lv, Y., &amp; Zhai, C. (2011). Lower-bounding term frequency normalization. In <em>Proceedings of the 20th ACM International Conference on Information and Knowledge Management</em> (pp. 7-16). ACM. <a href="https://doi.org/10.1145/2063576.2063581">https://doi.org/10.1145/2063576.2063581</a></li>
</ul>
</li>
<li><p><strong>Double Normalization TF-IDF</strong>：</p>
<ul>
<li>Jones, K. S., Walker, S., &amp; Robertson, S. E. (2000). A probabilistic model of information retrieval: development and comparative experiments: Part 2. <em>Information Processing &amp; Management, 36</em>(6), 809-840. <a href="https://doi.org/10.1016/S0306-4573(00)00016-9">https://doi.org/10.1016/S0306-4573(00)00016-9</a></li>
</ul>
</li>
<li><p><strong>TF-IDF with Class-Based Weighting</strong>：</p>
<ul>
<li>Debole, F., &amp; Sebastiani, F. (2003). Supervised term weighting for automated text categorization. In <em>Text mining and its applications</em> (pp. 81-97). Springer. <a href="https://doi.org/10.1007/3-540-36618-0_15">https://doi.org/10.1007/3-540-36618-0_15</a></li>
</ul>
</li>
<li><p><strong>TF-IDF with Query Expansion</strong>：</p>
<ul>
<li>Xu, J., &amp; Croft, W. B. (2000). Improving the effectiveness of information retrieval with local context analysis. <em>ACM Transactions on Information Systems (TOIS), 18</em>(1), 79-112. <a href="https://doi.org/10.1145/333135.333138">https://doi.org/10.1145/333135.333138</a></li>
</ul>
</li>
<li><p>**Smooth Inverse Frequency (SIF)**：</p>
<ul>
<li>Arora, S., Liang, Y., &amp; Ma, T. (2017). A simple but tough-to-beat baseline for sentence embeddings. <em>International Conference on Learning Representations (ICLR)</em>. Retrieved from <a href="https://openreview.net/pdf?id=SyK00v5xx">ICLR Conference</a>.</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
