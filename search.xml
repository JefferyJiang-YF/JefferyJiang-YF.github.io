<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Statistical Information in NLP</title>
    <url>/2024/07/09/Statistical-Information-in-NLP/</url>
    <content><![CDATA[<p>TF-IDF（Term Frequency-Inverse Document
Frequency）是一种常用的文本特征提取方法，用于评估一个词在一个文档中的重要性。除了标准的TF-IDF，还有一些变体和扩展来进一步优化特征提取过程。以下是一些常见的变体：</p>
<ol type="1">
<li><p><strong>BM25</strong>：</p>
<ul>
<li><p>BM25（Best Matching
25）是TF-IDF的一种改进版本，广泛用于信息检索。它考虑了词频饱和现象和文档长度规范化，使得对长文档和短文档的处理更加平衡。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{BM25}(q, D) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{f(q_i, D) \cdot
(k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot
\frac{|D|}{\text{avgdl}})}  
\]</span></p>
<p>其中，<span class="math inline">\(f(q_i, D)\)</span>是词<span
class="math inline">\(q_i\)</span>在文档D中的频率，|D|是文档长度，<span
class="math inline">\(\text{avgdl}\)</span>是平均文档长度，<span
class="math inline">\(k_1\)</span>和<span
class="math inline">\(b\)</span>是调节参数。</p></li>
<li><p><strong>TF-CHI</strong>：</p>
<ul>
<li><p>这种方法结合了词频（TF）和卡方检验（Chi-Square
Test，CHI），在考虑词频的同时，引入卡方统计量来衡量词与类别之间的相关性。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{TF-CHI}(t, c) = \text{TF}(t, c) \cdot \chi^2(t, c)  
\]</span></p>
<p>其中，<span class="math inline">\(\chi^2(t, c)\)</span>是词<span
class="math inline">\(t\)</span>和类别<span
class="math inline">\(c\)</span>之间的卡方统计量。</p></li>
<li><p><strong>TF-IG</strong>：</p>
<ul>
<li><p>结合了词频和信息增益（Information Gain,
IG），通过信息增益来评估词对分类的贡献。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{TF-IG}(t, c) = \text{TF}(t, c) \cdot \text{IG}(t, c)  
\]</span></p>
<p>其中，<span class="math inline">\(\text{IG}(t,
c)\)</span>是词t和类别c之间的信息增益。</p></li>
<li><p><strong>TF-RF</strong>：</p>
<ul>
<li><p>结合了词频和相关系数（Relevance Frequency,
RF），用于评估词与类别之间的相关性。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{TF-RF}(t, c) = \text{TF}(t, c) \cdot \text{RF}(t, c)  
\]</span></p>
<p>其中，<span class="math inline">\(\text{RF}(t,
c)\)</span>是词t在类别c中的相关系数。</p></li>
<li><p><strong>LDA-TF-IDF</strong>：</p>
<ul>
<li><p>将主题模型（LDA, Latent Dirichlet
Allocation）与TF-IDF结合，通过LDA生成主题分布后，再基于这些主题分布计算TF-IDF。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{LDA-TF-IDF}(t, D) = \text{TF-IDF}(t, D) \cdot P(z | D)  
\]</span></p>
<p>其中，<span class="math inline">\(P(z |
D)\)</span>是文档D中主题z的概率。</p></li>
<li><p><strong>Okapi BM25+</strong>：</p>
<ul>
<li><p>BM25的进一步改进版本，加入了一些附加参数来增强模型性能。</p></li>
<li><p>公式类似于BM25，但加入了文档饱和度和文档权重的参数。</p></li>
</ul></li>
<li><p><strong>Weighted TF-IDF</strong>：</p>
<ul>
<li><p>在计算TF-IDF时，对词频或逆文档频率进行加权。例如，可以根据词的词性、词的重要性、领域专有词等因素进行加权。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{Weighted TF-IDF}(t, d) = w_t \cdot \left( \frac{\text{TF}(t,
d)}{\text{DF}(t)} \right)  
\]</span></p>
<p>其中，<span
class="math inline">\(w_t\)</span>是词t的权重，可以根据不同的标准来设定。</p></li>
<li><p><strong>Logarithmic TF-IDF</strong>：</p>
<ul>
<li><p>使用对数变换来平滑词频，减小高频词对模型的影响。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{Log-TF}(t, d) = \log(1 + \text{TF}(t, d))  
\]</span></p>
<p><span class="math display">\[  
\text{Log-TF-IDF}(t, d) = \text{Log-TF}(t, d) \cdot \text{IDF}(t)  
\]</span></p></li>
<li><p><strong>Sublinear TF-IDF</strong>：</p>
<ul>
<li><p>对词频进行子线性缩放，通常用于大规模文档集。</p></li>
<li><p>公式：</p></li>
</ul>
<p><span class="math display">\[  
\text{Sublinear-TF}(t, d) = 1 + \log(\text{TF}(t, d))  
\]</span></p>
<p><span class="math display">\[  
\text{Sublinear-TF-IDF}(t, d) = \text{Sublinear-TF}(t, d) \cdot
\text{IDF}(t)  
\]</span></p></li>
<li><p><strong>Double Normalization TF-IDF</strong>：</p></li>
</ol>
<ul>
<li><p>通过双重归一化对词频进行标准化，常用于处理文档长度差异。</p></li>
<li><p>公式：</p></li>
</ul>
<p> <span class="math display">\[  
  \text{DoubleNorm-TF}(t, d) = 0.5 + 0.5 \cdot \frac{\text{TF}(t,
d)}{\max_{t&#39;} \text{TF}(t&#39;, d)}  
 \]</span><br />
 <span class="math display">\[  
  \text{DoubleNorm-TF-IDF}(t, d) = \text{DoubleNorm-TF}(t, d) \cdot
\text{IDF}(t)  
 \]</span></p>
<ol start="11" type="1">
<li><strong>TF-IDF with Class-Based Weighting</strong>：</li>
</ol>
<ul>
<li><p>根据词在不同类别中的分布情况进行加权，以增强分类任务的效果。</p></li>
<li><p>公式：</p></li>
</ul>
<p> <span class="math display">\[  
  \text{Class-TF-IDF}(t, d, c) = \text{TF-IDF}(t, d) \cdot
\text{ClassWeight}(t, c)  
 \]</span><br />
  其中，<span class="math inline">\(\text{ClassWeight}(t,
c)\)</span>表示词<span class="math inline">\(t\)</span>在类别<span
class="math inline">\(c\)</span>中的重要性权重。</p>
<ol start="12" type="1">
<li><strong>TF-IDF with Query Expansion</strong>：</li>
</ol>
<ul>
<li><p>在查询扩展过程中使用TF-IDF，对初始查询进行扩展，以包含更多相关的词。</p></li>
<li><p>公式：</p></li>
</ul>
<p> <span class="math display">\[  
  \text{Expanded-TF-IDF}(q, d) = \sum_{t \in q \cup E(q)}
\text{TF-IDF}(t, d)  
 \]</span><br />
  其中，<span class="math inline">\(E(q)\)</span>是查询<span
class="math inline">\(q\)</span>的扩展词集合。</p>
<ol start="13" type="1">
<li><strong>Smooth Inverse Frequency (SIF)</strong>：</li>
</ol>
<ul>
<li><p>用于词向量加权，通过平滑逆频率来减少高频词的影响。</p></li>
<li><p>公式：</p></li>
</ul>
<p> <span class="math display">\[  
  \text{SIF}(t) = \frac{a}{a + \text{TF}(t)}  
 \]</span><br />
  其中，<span class="math inline">\(a\)</span>是一个小的平滑参数。</p>
<h1 id="reference">Reference</h1>
<ol type="1">
<li><p><strong>BM25</strong>：</p>
<ul>
<li>Robertson, S. E., Walker, S., Beaulieu, M. M., Gatford, M., &amp;
Payne, A. (1996). Okapi at TREC-4. In D. K. Harman (Ed.), <em>NIST
Special Publication 500-236: Proceedings of The Fourth Text REtrieval
Conference (TREC-4)</em> (pp. 73-96). Gaithersburg, MD: National
Institute of Standards and Technology. Retrieved from <a
href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_921">SpringerLink</a>.</li>
</ul></li>
<li><p><strong>TF-CHI</strong>：</p>
<ul>
<li>Yang, Y., &amp; Pedersen, J. O. (1997). A comparative study on
feature selection in text categorization. In <em>Proceedings of the
Fourteenth International Conference on Machine Learning (ICML)</em> (pp.
412-420). San Francisco, CA: Morgan Kaufmann.</li>
</ul></li>
<li><p><strong>TF-IG</strong>：</p>
<ul>
<li>Yang, Y., &amp; Pedersen, J. O. (1997). A comparative study on
feature selection in text categorization. In <em>Proceedings of the
Fourteenth International Conference on Machine Learning (ICML)</em> (pp.
412-420). San Francisco, CA: Morgan Kaufmann.</li>
</ul></li>
<li><p><strong>TF-RF</strong>：</p>
<ul>
<li>Lan, M., Tan, C. L., Su, J., &amp; Lu, Y. (2009). Supervised and
traditional term weighting methods for automatic text categorization.
<em>IEEE Transactions on Pattern Analysis and Machine Intelligence,
31</em>(4), 721-735. <a
href="https://doi.org/10.1109/TPAMI.2008.110">https://doi.org/10.1109/TPAMI.2008.110</a></li>
</ul></li>
<li><p><strong>LDA-TF-IDF</strong>：</p>
<ul>
<li>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent Dirichlet
Allocation. <em>Journal of Machine Learning Research, 3</em>, 993-1022.
Retrieved from <a
href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">JMLR</a>.</li>
</ul></li>
<li><p><strong>Okapi BM25+</strong> Robertson, S., Zaragoza, H., &amp;
Taylor, M. (2004). Simple BM25 extension to multiple weighted fields. In
<em>Proceedings of the Thirteenth ACM International Conference on
Information and Knowledge Management</em> (pp. 42-49). ACM. <a
href="https://doi.org/10.1145/1031171.1031181">https://doi.org/10.1145/1031171.1031181</a></p></li>
<li><p><strong>Weighted TF-IDF</strong>：</p>
<ul>
<li>Baeza-Yates, R., &amp; Ribeiro-Neto, B. (1999). <em>Modern
Information Retrieval</em>. Addison-Wesley Longman Publishing Co.,
Inc.</li>
</ul></li>
<li><p><strong>Logarithmic TF-IDF</strong>：</p>
<ul>
<li>Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008).
<em>Introduction to Information Retrieval</em>. Cambridge University
Press. <a
href="https://doi.org/10.1017/CBO9780511809071">https://doi.org/10.1017/CBO9780511809071</a></li>
</ul></li>
<li><p><strong>Sublinear TF-IDF</strong>：</p>
<ul>
<li>Lv, Y., &amp; Zhai, C. (2011). Lower-bounding term frequency
normalization. In <em>Proceedings of the 20th ACM International
Conference on Information and Knowledge Management</em> (pp. 7-16). ACM.
<a
href="https://doi.org/10.1145/2063576.2063581">https://doi.org/10.1145/2063576.2063581</a></li>
</ul></li>
<li><p><strong>Double Normalization TF-IDF</strong>：</p>
<ul>
<li>Jones, K. S., Walker, S., &amp; Robertson, S. E. (2000). A
probabilistic model of information retrieval: development and
comparative experiments: Part 2. <em>Information Processing &amp;
Management, 36</em>(6), 809-840. <a
href="https://doi.org/10.1016/S0306-4573(00)00016-9">https://doi.org/10.1016/S0306-4573(00)00016-9</a></li>
</ul></li>
<li><p><strong>TF-IDF with Class-Based Weighting</strong>：</p>
<ul>
<li>Debole, F., &amp; Sebastiani, F. (2003). Supervised term weighting
for automated text categorization. In <em>Text mining and its
applications</em> (pp. 81-97). Springer. <a
href="https://doi.org/10.1007/3-540-36618-0_15">https://doi.org/10.1007/3-540-36618-0_15</a></li>
</ul></li>
<li><p><strong>TF-IDF with Query Expansion</strong>：</p>
<ul>
<li>Xu, J., &amp; Croft, W. B. (2000). Improving the effectiveness of
information retrieval with local context analysis. <em>ACM Transactions
on Information Systems (TOIS), 18</em>(1), 79-112. <a
href="https://doi.org/10.1145/333135.333138">https://doi.org/10.1145/333135.333138</a></li>
</ul></li>
<li><p><strong>Smooth Inverse Frequency (SIF)</strong>：</p>
<ul>
<li>Arora, S., Liang, Y., &amp; Ma, T. (2017). A simple but
tough-to-beat baseline for sentence embeddings. <em>International
Conference on Learning Representations (ICLR)</em>. Retrieved from <a
href="https://openreview.net/pdf?id=SyK00v5xx">ICLR Conference</a>.</li>
</ul></li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/07/02/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>hexo</category>
        <category>theme</category>
      </categories>
      <tags>
        <tag>hello</tag>
        <tag>world</tag>
      </tags>
  </entry>
  <entry>
    <title>第一篇文章</title>
    <url>/2024/07/07/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<h1 id="hexo-博客-新建文章">Hexo 博客 新建文章</h1>
<p>输入如下命令，创建一篇新的文章</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>layout 是文章的布局，默认为post，可以先不写。</p></li>
<li><p>title
是文章的标题，也是文件的名字，存储在source/_posts下。</p></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@localhost blog]#hexo new &quot;我的第一篇文章&quot;</span><br><span class="line">INFO  Validating config</span><br><span class="line">INFO  Created: D:\blog_hexo\myblog\source\_posts\我的第一篇文章.md</span><br></pre></td></tr></table></figure>
<p>可以看到创建的文件在<code>\source\_posts</code>路径下，之后在MarkDown编辑器上打开就可以编辑你的文章内容。
打开创建的MarkDown文件，默认内容如下，我们可以在新建的文件里用MarkDown语法编辑文章</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 我的第一篇文章</span><br><span class="line">date: 2022-03-15 17:57:05</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<h1 id="布局">布局</h1>
<p>Hexo 有三种默认布局：<code>post</code>、<code>page</code> 和
<code>draft</code>在创建者三种不同类型的文件时，它们将会被保存到不同的路径。
- post - 默认布局，用于文章。 - 保存在<code>source/_posts</code>目录下。
- page - 用于单页。 - 保存在<code>source</code>目录下。 - draft -
用于草稿。 - 保存在<code>source/_drafts</code>目录下。
如果你不想你的文章被处理，你可以将 Front-Matter 中的layout: 设为 false
。</p>
<p>Front-matter 参数 Front-matter
是文章最上方以—分隔的区域，用于指定文章的变量设置 <img data-src="https://tuchuang.org.cn/imgs/2024/07/06/9dd63d3ad75489a1.png" /></p>
<h1 id="next主题-多层级分类">NexT主题 多层级分类</h1>
<p>1.单层分类 如果你的一篇文章只分一类，可以像如下方式设置分类名：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- Hexo 博客</span><br></pre></td></tr></table></figure> 2.父子分类
如果你想让一篇文章处于父类中的子类中，可以像如下方式设置分类名：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- 前端</span><br><span class="line">- 笔记</span><br></pre></td></tr></table></figure> 3.并列分类
如果你想让一篇文章处于同一层的不同类中，可以像如下方式设置分类名：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [后端]</span><br><span class="line">- [笔记]</span><br></pre></td></tr></table></figure> 4.同一父类不同子类
如果你想让一篇文章处于同一父类的不同类中，可以像如下方式设置分类名：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories:</span><br><span class="line">- [学习,html]</span><br><span class="line">- [学习,http]</span><br></pre></td></tr></table></figure> 总体效果图： <img data-src="https://tuchuang.org.cn/imgs/2024/07/06/9b053c5df042c6bf.png" /></p>
]]></content>
  </entry>
</search>
