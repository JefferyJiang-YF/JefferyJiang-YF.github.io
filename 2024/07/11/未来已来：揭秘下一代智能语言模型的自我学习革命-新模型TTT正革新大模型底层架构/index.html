<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/fire.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/fire.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/fire.png">
  <link rel="mask-icon" href="/images/fire.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Georgia:300,300italic,400,400italic,700,700italic%7CConsolas:300,300italic,400,400italic,700,700italic%7C"Courier+New":300,300italic,400,400italic,700,700italic%7Cmonospace:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"jefferyjiang-yf.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"giscus","storage":true,"lazyload":true,"nav":null,"activeClass":"giscus"},"stickytabs":true,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":true,"preload":true}}</script><script src="/js/config.js"></script>

    <meta name="description" content="导读： 在这个信息爆炸的时代，人工智能正以前所未有的速度进化。想象一下，如果我们的智能助手能够像人类一样，通过不断学习来适应每一个新的挑战，那将会怎样？这不是科幻小说的情节，而是正在我们眼前发生的科技革命！ 今天，我要带你深入了解一篇突破性的科研论文——《Learning to (Learn at Test Time): RNNs with Expressive Hidden States》。这">
<meta property="og:type" content="article">
<meta property="og:title" content="TTT人话版解读-Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States">
<meta property="og:url" content="https://jefferyjiang-yf.github.io/2024/07/11/%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E6%8F%AD%E7%A7%98%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E9%9D%A9%E5%91%BD-%E6%96%B0%E6%A8%A1%E5%9E%8BTTT%E6%AD%A3%E9%9D%A9%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/index.html">
<meta property="og:site_name" content="Jeffrey Blog">
<meta property="og:description" content="导读： 在这个信息爆炸的时代，人工智能正以前所未有的速度进化。想象一下，如果我们的智能助手能够像人类一样，通过不断学习来适应每一个新的挑战，那将会怎样？这不是科幻小说的情节，而是正在我们眼前发生的科技革命！ 今天，我要带你深入了解一篇突破性的科研论文——《Learning to (Learn at Test Time): RNNs with Expressive Hidden States》。这">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic.imgdb.cn/item/668eb9c9d9c307b7e923e025.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/668eb9a3d9c307b7e923ab3c.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/668eb1b2d9c307b7e9193ea1.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/668eb1d4d9c307b7e9196878.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/668eb203d9c307b7e9199fd8.png">
<meta property="article:published_time" content="2024-07-10T23:59:59.000Z">
<meta property="article:modified_time" content="2024-07-10T16:43:56.476Z">
<meta property="article:author" content="Jeffrey Jiang">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="自回归模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/668eb9c9d9c307b7e923e025.png">


<link rel="canonical" href="https://jefferyjiang-yf.github.io/2024/07/11/%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E6%8F%AD%E7%A7%98%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E9%9D%A9%E5%91%BD-%E6%96%B0%E6%A8%A1%E5%9E%8BTTT%E6%AD%A3%E9%9D%A9%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://jefferyjiang-yf.github.io/2024/07/11/%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E6%8F%AD%E7%A7%98%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E9%9D%A9%E5%91%BD-%E6%96%B0%E6%A8%A1%E5%9E%8BTTT%E6%AD%A3%E9%9D%A9%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/","path":"2024/07/11/未来已来：揭秘下一代智能语言模型的自我学习革命-新模型TTT正革新大模型底层架构/","title":"TTT人话版解读-Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TTT人话版解读-Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States | Jeffrey Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Jeffrey Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record the growth of life</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-主页|home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页|Home</a></li><li class="menu-item menu-item-关于|about"><a href="/academic/" rel="section"><i class="fa fa-user fa-fw"></i>关于|About</a></li><li class="menu-item menu-item-标签|tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签|Tags</a></li><li class="menu-item menu-item-分类|categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类|Categories</a></li><li class="menu-item menu-item-归档|archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档|Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ttt%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">2.</span> <span class="nav-text">TTT的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">2.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.2.</span> <span class="nav-text">如何工作</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E4%BC%A0%E7%BB%9F%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">对比传统模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn"><span class="nav-number">3.1.</span> <span class="nav-text">1. 循环神经网络（RNN）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6sa"><span class="nav-number">3.2.</span> <span class="nav-text">2. 自注意力机制（SA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#test-time-trainingttt%E5%B1%82"><span class="nav-number">3.3.</span> <span class="nav-text">3. Test-Time
Training（TTT）层</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AF%8F%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AD%A5%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E7%B1%BB%E6%AF%94%E8%A7%A3%E9%87%8A"><span class="nav-number">4.</span> <span class="nav-text">每个时间步复杂度的类比解释</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%85%E4%BA%8B%E6%AF%94%E5%96%BB"><span class="nav-number">4.1.</span> <span class="nav-text">故事比喻：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rnns-sa%E5%92%8Cttt%E7%BB%BC%E5%90%88%E6%AF%94%E8%BE%83"><span class="nav-number">4.2.</span> <span class="nav-text">RNNs, SA和TTT综合比较</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%92%88%E5%AF%B9rnn%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8Cttt%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">4.2.1.</span> <span class="nav-text">针对RNN的问题和TTT的解决方案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%92%88%E5%AF%B9sa%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8Cttt%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">4.2.2.</span> <span class="nav-text">针对SA的问题和TTT的解决方案</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ttt%E5%AF%B9rnn%E7%9A%84%E6%94%B9%E8%BF%9B-%E6%9E%B6%E6%9E%84%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%A6%E9%83%A8%E5%88%86"><span class="nav-number">5.</span> <span class="nav-text">TTT对RNN的改进
(架构以及数学部分)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ascii-%E5%9B%BE%E5%83%8F%E5%B1%95%E7%A4%BA"><span class="nav-number">5.1.</span> <span class="nav-text">ASCII 图像展示</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E7%9A%84rnns%E6%9E%B6%E6%9E%84"><span class="nav-number">5.1.1.</span> <span class="nav-text">传统的RNNs架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ttt%E6%9E%B6%E6%9E%84"><span class="nav-number">5.1.2.</span> <span class="nav-text">TTT架构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E8%AF%B4%E6%98%8E"><span class="nav-number">5.2.</span> <span class="nav-text">公式说明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rnns"><span class="nav-number">5.2.1.</span> <span class="nav-text">RNNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ttt"><span class="nav-number">5.2.2.</span> <span class="nav-text">TTT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E5%AE%83%E4%BB%AC%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">5.3.</span> <span class="nav-text">简单语言描述它们的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn-recurrent-neural-network"><span class="nav-number">5.3.1.</span> <span class="nav-text">RNN (Recurrent Neural
Network)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ttt-test-time-training"><span class="nav-number">5.3.2.</span> <span class="nav-text">TTT (Test-Time Training)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB"><span class="nav-number">5.3.3.</span> <span class="nav-text">主要区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#qa%E9%82%A3%E4%B9%88ttt%E5%92%8C%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%E4%BD%BF%E7%94%A8train%E6%A8%A1%E5%BC%8F%E7%9A%84rnn%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="nav-number">6.</span> <span class="nav-text">[QA]那么TTT和测试阶段使用Train模式的RNN有什么区别?</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%AE%E7%9A%84%E5%92%8C%E7%AD%96%E7%95%A5"><span class="nav-number">6.1.</span> <span class="nav-text">目的和策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%A7%E7%BB%AD%E8%AE%AD%E7%BB%83rnn"><span class="nav-number">6.1.1.</span> <span class="nav-text">继续训练RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ttt-1"><span class="nav-number">6.1.2.</span> <span class="nav-text">TTT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%96%BD%E6%96%B9%E5%BC%8F%E5%92%8C%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">6.2.</span> <span class="nav-text">实施方式和适用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%A7%E7%BB%AD%E8%AE%AD%E7%BB%83rnn-1"><span class="nav-number">6.2.1.</span> <span class="nav-text">继续训练RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ttt-2"><span class="nav-number">6.2.2.</span> <span class="nav-text">TTT</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ttt%E5%AF%B9sa%E7%9A%84%E6%94%B9%E8%BF%9B%E6%9E%B6%E6%9E%84%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%A6%E9%83%A8%E5%88%86"><span class="nav-number">7.</span> <span class="nav-text">TTT对SA的改进(架构以及数学部分)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E7%9A%84%E8%A1%A8%E8%BE%BE"><span class="nav-number">7.1.</span> <span class="nav-text">数学的表达</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-sa"><span class="nav-number">7.1.1.</span> <span class="nav-text">Self-Attention (SA)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#test-time-training-ttt"><span class="nav-number">7.1.2.</span> <span class="nav-text">Test-Time Training (TTT)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB-1"><span class="nav-number">7.1.3.</span> <span class="nav-text">主要区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#qa%E4%BB%8E%E5%A4%B4%E8%AE%AD%E7%BB%83rnnsa%E5%92%8Cttt%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="nav-number">8.</span> <span class="nav-text">[QA]从头训练,RNN,SA和TTT有什么区别?</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ttt%E5%B1%82%E7%9A%84%E6%9B%B4%E6%96%B0%E8%A7%84%E5%88%99"><span class="nav-number">8.1.</span> <span class="nav-text">TTT层的更新规则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8E%E4%BC%A0%E7%BB%9Frnn%E5%92%8Csa%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">8.2.</span> <span class="nav-text">与传统RNN和SA的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9Frnn"><span class="nav-number">8.2.1.</span> <span class="nav-text"> 传统RNN：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6sa-1"><span class="nav-number">8.2.2.</span> <span class="nav-text">自注意力机制（SA）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ttt%E5%B1%82"><span class="nav-number">8.2.3.</span> <span class="nav-text">TTT层：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#qa%E9%82%A3%E4%B9%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E8%AE%AD%E7%BB%83ttt%E7%9A%84%E6%97%B6%E5%80%99%E5%BF%85%E9%A1%BB%E8%A6%81%E6%9C%89%E9%AA%8C%E8%AF%81%E9%9B%86%E6%89%8D%E8%83%BD%E6%BF%80%E6%B4%BB%E9%9A%90%E8%97%8F%E5%B1%82%E8%87%AA%E9%80%82%E5%BA%94%E6%9B%B4%E6%96%B0%E7%9A%84%E8%83%BD%E5%8A%9B%E5%90%97"><span class="nav-number">9.</span> <span class="nav-text">[QA]那么也就是说,训练TTT的时候,必须要有验证集,才能激活隐藏层自适应更新的能力吗?</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ttt%E5%B1%82%E6%9B%B4%E6%96%B0%E6%9C%BA%E5%88%B6%E7%9A%84%E7%90%86%E8%A7%A3"><span class="nav-number">9.1.</span> <span class="nav-text">TTT层更新机制的理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9C%80%E6%B1%82"><span class="nav-number">9.2.</span> <span class="nav-text">数据需求</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E8%80%83%E8%99%91"><span class="nav-number">9.2.1.</span> <span class="nav-text">实际应用考虑</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9B%B4%E5%A4%9A%E8%AF%B7%E5%8F%82%E8%80%83%E5%8E%9F%E8%AE%BA%E6%96%87-%E9%93%BE%E6%8E%A5"><span class="nav-number">10.</span> <span class="nav-text">更多请参考原论文-链接</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jeffrey Jiang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jeffrey Jiang</p>
  <div class="site-description" itemprop="description">The blog of Jeffery Jiang</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/JefferyJiang-YF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;JefferyJiang-YF" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/905152222jyf@gmail.com" title="E-Mail → 905152222jyf@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/07/12/Comparasion-the-GPTs-with-scholar-target-used-in-CN-and-EN-prompt/" title="2024&#x2F;07&#x2F;12&#x2F;Comparasion-the-GPTs-with-scholar-target-used-in-CN-and-EN-prompt&#x2F;">Comparasion the GPTs with scholar-target used in CN and EN prompt</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/07/11/%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E6%8F%AD%E7%A7%98%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E9%9D%A9%E5%91%BD-%E6%96%B0%E6%A8%A1%E5%9E%8BTTT%E6%AD%A3%E9%9D%A9%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/" title="2024&#x2F;07&#x2F;11&#x2F;未来已来：揭秘下一代智能语言模型的自我学习革命-新模型TTT正革新大模型底层架构&#x2F;">TTT人话版解读-Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/07/09/Normalization-techniques-in-Deep-Learning/" title="2024&#x2F;07&#x2F;09&#x2F;Normalization-techniques-in-Deep-Learning&#x2F;">Normalization techniques in Deep Learning</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/07/09/Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States/" title="2024&#x2F;07&#x2F;09&#x2F;Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States&#x2F;">Learning to Learn at Test Time RNNs with Expressive Hidden States</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/07/09/Statistical-Information-in-NLP/" title="2024&#x2F;07&#x2F;09&#x2F;Statistical-Information-in-NLP&#x2F;">Statistical Information in NLP</a>
        </li>
    </ul>
  </div>
        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jefferyjiang-yf.github.io/2024/07/11/%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E6%8F%AD%E7%A7%98%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E9%9D%A9%E5%91%BD-%E6%96%B0%E6%A8%A1%E5%9E%8BTTT%E6%AD%A3%E9%9D%A9%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jeffrey Jiang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jeffrey Blog">
      <meta itemprop="description" content="The blog of Jeffery Jiang">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="TTT人话版解读-Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States | Jeffrey Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TTT人话版解读-Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-07-10 23:59:59 / Modified: 16:43:56" itemprop="dateCreated datePublished" datetime="2024-07-10T23:59:59Z">2024-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">自回归模型</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>9 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><img data-src="https://pic.imgdb.cn/item/668eb9c9d9c307b7e923e025.png" />
<strong>导读：</strong>
在这个信息爆炸的时代，人工智能正以前所未有的速度进化。想象一下，如果我们的智能助手能够像人类一样，通过不断学习来适应每一个新的挑战，那将会怎样？这不是科幻小说的情节，而是正在我们眼前发生的科技革命！</p>
<p>今天，我要带你深入了解一篇突破性的科研论文——《Learning to (Learn at
Test Time): RNNs with Expressive Hidden
States》。这篇论文不仅仅是学术界的一次飞跃，更是预示着我们与机器交流方式的重大转变。</p>
<p>论文中介绍了一种新型的序列建模层——Test-Time
Training（TTT）层，它让机器在测试时也能进行自我学习，就像是一个学生在考试中不断吸取教训，越做越好。这听起来是不是有点像天方夜谭？但别急，接下来我将用最通俗易懂的语言，为你揭开TTT层的神秘面纱，一起见证人工智能如何变得更加智能，更加接近人类学习的本质。</p>
<p>准备好了吗？让我们一起探索这场人工智能的自我学习革命，看看它是如何让机器在处理长文本时表现得更加出色，甚至在某些情况下，超越了当前最顶尖的Transformer模型和Mamba
RNN。这不仅仅是技术的突破，更是对智能本质的一次深刻洞察。跟随我，让我们一探究竟！</p>
<h1 id="引言">引言</h1>
<p>在人工智能和机器学习领域，我们经常遇到的一个挑战是如何让模型不仅在训练数据上表现良好，而且在现实世界的多变环境中也能保持稳定和高效的性能。
传统的机器学习流程通常包括两个阶段：<mark style="background: #FF5582A6;">训练和测试</mark>。
在训练阶段，模型通过学习大量的数据来获得尽可能多的信息；
在测试阶段，模型则需要在未见过的数据上进行预测。<strong>然而，这种分离的训练测试方法有时候会导致模型对新环境的适应性不足</strong>，尤其是在数据分布发生变化时。为了解决这一问题，出现了一种新的技术——<strong>测试时训练（Test-Time
Training, TTT），它旨在提高模型的适应性和泛化能力。</strong></p>
<h1 id="ttt的基本概念">TTT的基本概念</h1>
<h2 id="定义">定义</h2>
<p>TTT是一种在模型的测试阶段<strong>继续学习的技术</strong>，它允许模型在实际使用中根据新的输入数据进行自我调整。<strong>这种方法不同于传统的机器学习流程，其中模型一旦完成训练，其参数就固定不变，无法适应测试数据中的新特征或新分布。</strong></p>
<p><img data-src="https://pic.imgdb.cn/item/668eb9a3d9c307b7e923ab3c.png" /></p>
<h2 id="如何工作">如何工作</h2>
<p>TTT的核心思想是在<strong>模型进行预测时，同时进行微调</strong>。具体来说，这意味着在模型对测试数据进行分类或回归前，先用同样的测试数据微调模型的参数。这种策略可以使模型更好地适应当前的数据环境，尤其是在面对数据分布变化时。</p>
<h1 id="对比传统模型">对比传统模型</h1>
<p><img data-src="https://pic.imgdb.cn/item/668eb1b2d9c307b7e9193ea1.png" />
在传统的机器学习模型中，<strong>训练阶段和测试阶段是严格分开的</strong>。一旦模型在训练集上训练完成，它的参数就被固定下来，然后用于评估测试集的表现。这种方法的主要问题是它假设训练数据和测试数据是同分布的。然而，在真实世界的应用中，这种假设往往不成立。而TTT通过在测试时调整模型，使得模型能够适应那些在训练阶段未曾见过的数据分布，从而提高了模型的泛化能力和实用性。</p>
<p>通过引入TTT，我们可以使模型在面对新的挑战时更加灵活和鲁棒。接下来的部分将具体探讨TTT如何改进特定类型的神经网络模型，如<strong>循环神经网络（RNN）和自注意力机制（SA）。</strong></p>
<p>在计算模型如RNN、SA（自注意力机制，如Transformer）和TTT（Test-Time
Training）层的时间复杂度时，我们主要关注模型处理单个输入序列或在单个时间步中执行的计算量。这些复杂度通常取决于模型的结构、输入序列的长度以及模型的参数配置。
<img data-src="https://pic.imgdb.cn/item/668eb1d4d9c307b7e9196878.png" />
在处理长序列数据时，循环神经网络（RNN）和自注意力（SA）机制各自的表现特点及其时间复杂度的优化需求有所不同。以下是对三种序列模型层——RNN、自注意力和Test-Time
Training（TTT）层——在长序列上的性能和时间复杂度表现的进一步分析和比较：</p>
<h2 id="循环神经网络rnn">1. <strong>循环神经网络（RNN）</strong></h2>
<ul>
<li><strong>性能</strong>：RNN通过将历史信息压缩进一个固定大小的隐藏状态来处理序列数据，这种压缩机制使得对于长序列而言，其表达能力可能受限。随着序列长度的增加，固定大小的隐藏状态可能无法有效捕捉到所有历史信息的细节和复杂性。</li>
<li><strong>时间复杂度</strong>：对于每个输入标记，RNN的更新规则和输出规则的时间复杂度通常是常数时间，即
<span
class="math inline">\(O(1)\)</span>，但在实际操作中，每个时间步涉及的计算（如权重矩阵乘法）的复杂度是
<span class="math inline">\(O(d^2)\)</span>，其中 <span
class="math inline">\(d\)</span> 是隐藏状态的维度。因此，对于长度为
<span class="math inline">\(n\)</span> 的序列，整个序列的处理复杂度为
<span class="math inline">\(O(nd^2)\)</span>。</li>
</ul>
<h2 id="自注意力机制sa">2. <strong>自注意力机制（SA）</strong></h2>
<ul>
<li><strong>性能</strong>：自注意力机制通过将每个输入存储于一个不断增长的Key-Value列表中，不压缩历史上下文，从而在处理长序列时更具表达力。每个输入都可以直接与之前的所有输入关联，这使得该模型在捕捉长距离依赖关系时表现出色。</li>
<li><strong>时间复杂度</strong>：尽管自注意力机制在表达力上优越，但其时间复杂度为
<span
class="math inline">\(O(n^2d)\)</span>，这是因为要计算所有输入对之间的关系，每增加一个输入，计算量呈二次方增长。</li>
</ul>
<h2 id="test-time-trainingttt层">3. <strong>Test-Time
Training（TTT）层</strong></h2>
<ul>
<li><strong>性能</strong>：TTT层通过在每个时间步使用当前输入和前一步的参数更新权重，尝试在测试时继续训练模型来改进表现。这种策略可能有助于适应或捕捉序列中出现的新模式。</li>
<li><strong>时间复杂度</strong>：虽然每个时间步的更新成本是常数级的，即
<span
class="math inline">\(O(1)\)</span>，但这取决于具体实现和权重更新策略的复杂度。如果采用更复杂的模型（如多层感知机MLP），计算复杂度可能更高，但设计良好的TTT层可以保持较低的复杂度，比如
<span class="math inline">\(O(nd)\)</span> 或 <span
class="math inline">\(O(nd^2)\)</span>。</li>
</ul>
<hr />
<h1 id="每个时间步复杂度的类比解释">每个时间步复杂度的类比解释</h1>
<p>解释RNN和TTT的时间复杂度为 <span class="math inline">\(O(1)\)</span>
而自注意力机制（SA）的时间复杂度为 <span
class="math inline">\(O(t)\)</span>
时，我们可以使用简单的比喻，使<strong>小学生</strong>也能理解这些复杂的概念。以下是一种可能的解释方法：</p>
<h2 id="故事比喻">故事比喻：</h2>
<p>想象一下，你在做一个长长的拼图。每个拼图块都是一个输入信息（比如一天的记忆），而你需要决定每个拼图块放在哪里才能把整个图案拼好。</p>
<p><strong>RNN（循环神经网络）和T TT（Test-Time Training）：</strong> -
<strong>比喻</strong>：假设你每拿到一个新的拼图块，你只会看一下前一个拼图块，然后决定新的拼图块应该放在哪里。这样，你每次处理一个拼图块的速度都很快，因为你只查看一小部分信息（即前一个状态）。
-
<strong>数学表示</strong>：这就像是RNN的更新规则，你只需要根据上一个状态
<span class="math inline">\(s_{t-1}\)</span> 和当前的输入 <span
class="math inline">\(x_t\)</span> 来更新当前的状态 <span
class="math inline">\(s_t\)</span>。这个过程很快，不管拼图有多长，每次处理的时间都差不多，所以我们说它的时间复杂度是
<span class="math inline">\(O(1)\)</span>，即常数时间。</p>
<p><strong>SA（自注意力机制）：</strong> -
<strong>比喻</strong>：现在，假设你每拿到一个新的拼图块，你需要重新看一遍你已经放下的所有拼图块，以决定新的拼图块应该放在哪里。这意味着，随着你放下的拼图块越来越多，你花在每个新拼图块上的时间也越来越长。
-
<strong>数学表示</strong>：在自注意力机制中，每次计算新的输出时，你需要考虑到之前所有的输入信息（所有的Key-Value对），这使得处理每个新输入的时间随着输入数量的增加而增加。因此，我们说自注意力的时间复杂度是
<span class="math inline">\(O(t)\)</span>，即随时间线性增长。</p>
<hr />
<h2 id="rnns-sa和ttt综合比较">RNNs, SA和TTT综合比较</h2>
<p>在处理长序列时，<strong>RNN因其固定大小的状态而在表达复杂历史信息上受限</strong>，而自<strong>注意力由于其能够显式存储并处理所有历史信息而具有更高的表达力</strong>，但<strong>代价是显著增加的计算复杂度</strong>。TTT层提供了一个在实际应用中可能有用的<strong><em>折中</em></strong>方案，通过<u>适应性更新模型参数来尝试捕捉序列中的关系，但其效能和效果可能依赖于具体的实现和应用场景</u>。</p>
<p>因此，在设计序列处理模型时，选择合适的模型类型需要根据应用的具体需求，考虑到性能、计算资源和序列的特性平衡这些因素。</p>
<h3 id="针对rnn的问题和ttt的解决方案">针对RNN的问题和TTT的解决方案</h3>
<p><strong>RNN的问题</strong>:</p>
<ol type="1">
<li><strong>过拟合</strong>：RNN容易过拟合于训练数据，特别是在数据集小或者序列很长时。</li>
<li><strong>时间依赖性</strong>：RNN对序列中时间步的远近敏感，可能导致对早期输入的信息忽略，即所谓的长期依赖问题。</li>
<li><strong>环境变化适应性</strong>：RNN通常在固定的数据分布上表现良好，但对于环境或数据分布的变化适应性较差。</li>
</ol>
<p><strong>TTT的解决方案</strong>:</p>
<ul>
<li><strong>动态调整</strong>：通过在测试时调整模型，TTT可以减少模型对于训练数据的过拟合问题，增强模型对新环境或数据分布的适应性。</li>
<li><strong>缓解长期依赖问题</strong>：通过在测试阶段动态更新模型参数，TTT可以帮助模型更好地捕捉到当前输入的重要性，从而可能间接缓解因长期依赖导致的信息丢失问题。</li>
</ul>
<h3 id="针对sa的问题和ttt的解决方案">针对SA的问题和TTT的解决方案</h3>
<p><strong>SA的问题</strong>:</p>
<ol type="1">
<li><strong>计算成本高</strong>：自注意力机制涉及到复杂的计算过程，尤其是在处理长序列时，其时间和空间复杂度都很高。</li>
<li><strong>泛化能力</strong>：虽然自注意力机制在处理不同长度的输入时具有较好的灵活性，但其在面对与训练数据分布不同的新数据时的泛化能力可能受限。</li>
<li><strong>对抗样本敏感</strong>：SA模型（如Transformer）可能对输入的微小变化敏感，导致其容易受到对抗样本的攻击。</li>
</ol>
<p><strong>TTT的解决方案</strong>:</p>
<ul>
<li><strong>提升泛化能力</strong>：通过在测试时根据实际输入进行模型更新，TTT可以有效提升模型对新数据的处理能力，从而增强其泛化性。</li>
<li><strong>增强鲁棒性</strong>：在测试时调整模型参数可以帮助模型更好地应对输入的微小变化，减少对抗样本的影响。</li>
</ul>
<hr />
<h1 id="ttt对rnn的改进-架构以及数学部分">TTT对RNN的改进
(架构以及数学部分)</h1>
<p>让我们从两个部分来展开这个解释：首先是通过ASCII图像展示出传统的递归神经网络（RNNs）和TTT（Test
Time
Training）模型的架构，然后用公式来说明它们的工作原理，并用简单的语言描述它们之间的区别。</p>
<h2 id="ascii-图像展示">ASCII 图像展示</h2>
<h3 id="传统的rnns架构">传统的RNNs架构</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+---------+      +----------+      +----------+</span><br><span class="line">|         |      |          |      |          |</span><br><span class="line">|  Input  +-----&gt;+ RNN Cell +-----&gt;+  Output  |</span><br><span class="line">|  Layer  |      |          |      |  Layer   |</span><br><span class="line">+---------+      +----------+      +----------+</span><br><span class="line">                    ^   |</span><br><span class="line">                    |   |</span><br><span class="line">                    +---+</span><br><span class="line">                Recurrent Connection</span><br></pre></td></tr></table></figure>
<h3 id="ttt架构">TTT架构</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">+---------+       +--------+       +----------+</span><br><span class="line">|         |       |        |       |          |</span><br><span class="line">|  Input  +------&gt;+ Task   +------&gt;+ Learner  |</span><br><span class="line">| Sequence|       |        |       |          |</span><br><span class="line">+----+----+       +---+----+       +-----+----+</span><br><span class="line">     |                |                  |</span><br><span class="line">     |                |                  | </span><br><span class="line">     |                |                  |</span><br><span class="line">     |          +-----v----+       +-----v----+</span><br><span class="line">     |          |          |       |          |</span><br><span class="line">     +---------&gt;+  Theta   |       |  Model   |</span><br><span class="line">                | (Params) |       | (Linear) |</span><br><span class="line">                |          |       |          |</span><br><span class="line">                +-----+----+       +----+-----+</span><br><span class="line">                      |                 |</span><br><span class="line">                      |                 |</span><br><span class="line">                      |      +----------v---------+</span><br><span class="line">                      |      |                    |</span><br><span class="line">                      +-----&gt;+  Update Parameters |</span><br><span class="line">                             |   (Online GD)      |</span><br><span class="line">                             |                    |</span><br><span class="line">                             +--------+-----------+</span><br><span class="line">                                      |</span><br><span class="line">                                      |</span><br><span class="line">                              +-------v-------+</span><br><span class="line">                              |               |</span><br><span class="line">                              |  Output       |</span><br><span class="line">                              |  Sequence     |</span><br><span class="line">                              |               |</span><br><span class="line">                              +---------------+</span><br></pre></td></tr></table></figure>
<p><img data-src="https://pic.imgdb.cn/item/668eb203d9c307b7e9199fd8.png" />
1. <strong><code>Task</code> 类定义</strong>：</p>
<pre><code>- `Task` 类中定义了三个参数：`theta_K`, `theta_V`, `theta_Q`。这些参数用于构建不同的视图（`train_view` 和 `label_view`），这种方式实际上是在创建数据的不同表示，而不直接依赖外部提供的**标签**。
- `loss` 函数计算的是 `train_view` 和 `label_view` 之间的均方误差（MSE）。注意，`label_view` 是通过模型参数 `theta_V` 直接从输入 `x` 计算得出的，而非传统意义上的外部提供的标签。</code></pre>
<ol start="2" type="1">
<li><p><strong><code>Learner</code> 类的实现</strong>：</p>
<ul>
<li><code>Learner</code> 类包含一个模型和一个优化器。该类的 <code>train</code> 方法用于计算模型关于当前任务损失的梯度，并使用这个梯度来更新模型参数。</li>
<li>重要的是，梯度的计算是基于 <code>Task</code> 的 <code>loss</code> 函数，该函数本身使用从数据生成的 <code>label_view</code> 来计算损失，而不是使用真正的标签数据。</li>
</ul></li>
<li><p><strong>数据驱动的学习过程</strong>：</p>
<ul>
<li>在整个学习过程中，<code>Learner</code> 使用的训练数据（由 <code>Task</code> 处理生成的 <code>train_view</code> 和 <code>label_view</code>）完全基于输入数据 <code>x</code> 的变换。这意味着模型的更新依赖于如何将输入数据转化为内部表示，而非外部的、独立的标签。</li>
</ul></li>
<li><p><strong>动态更新机制</strong>：</p>
<ul>
<li>在 <code>TTT_Layer</code> 的 <code>forward</code> 方法中，对于输入序列中的每个元素，都会调用 <code>Learner</code> 的 <code>train</code> 方法来更新状态，并使用更新后的状态进行预测。这种方式表明模型是在不断地从每个新的输入数据中学习并适应，而非仅仅在一开始使用固定的参数进行所有的预测。</li>
</ul></li>
</ol>
<h2 id="公式说明">公式说明</h2>
<h3 id="rnns">RNNs</h3>
<p>RNNs 通常通过下面的公式来更新其隐藏状态 <span
class="math inline">\(h_t\)</span>： <span class="math inline">\(h_t =
f(W \cdot h_{t-1} + U \cdot x_t + b)\)</span> 其中： - <span
class="math inline">\(h_t\)</span> 是时间步 <span
class="math inline">\(t\)</span> 的隐藏状态。 - <span
class="math inline">\(x_t\)</span> 是时间步 <span
class="math inline">\(t\)</span> 的输入。 - <span
class="math inline">\(W\)</span> 和 <span
class="math inline">\(U\)</span> 是权重矩阵。 - <span
class="math inline">\(b\)</span> 是偏置。 - <span
class="math inline">\(f\)</span> 是激活函数，如tanh或ReLU。</p>
<h3 id="ttt">TTT</h3>
<p>TTT 模型在测试时通过下面的方式更新模型参数： <span
class="math inline">\(\theta \leftarrow \theta - \eta \cdot
\nabla_\theta L(\theta, x, y)\)</span> 其中： - <span
class="math inline">\(\theta\)</span> 是模型参数。 - <span
class="math inline">\(L\)</span> 是损失函数，如均方误差。 - <span
class="math inline">\(\eta\)</span> 是学习率。 - <span
class="math inline">\(x\)</span> 和 <span
class="math inline">\(y\)</span> 分别是输入数据和标签。 - <span
class="math inline">\(\nabla_\theta L\)</span> 是关于参数 <span
class="math inline">\(\theta\)</span> 的损失梯度。</p>
<h2 id="简单语言描述它们的区别">简单语言描述它们的区别</h2>
<p><strong>传统RNNs</strong>： - RNNs
专注于通过其循环连接处理序列数据，利用前一步的隐藏状态来影响当前步的输出。
- 模型参数在训练阶段确定后，在测试时通常不再改变。</p>
<p><strong>TTT模型</strong>： - TTT
在模型部署后（即测试时）仍然继续学习和调整其参数，以更好地适应新的或变化的数据。
-
它通过在每个测试实例上应用在线学习方法来优化性能，适用于动态环境或实时更新的需求。
## 数学的表达
在这里，我们可以通过数学形式来展示循环神经网络（RNN）和测试时训练（TTT,
Test-Time
Training）的区别。两种模型都处理序列数据，但它们更新隐藏状态和产生输出的方式有所不同。</p>
<h3 id="rnn-recurrent-neural-network">RNN (Recurrent Neural
Network)</h3>
<p><strong>数学表示</strong>: - <strong>初始状态</strong>: <span
class="math inline">\(s_0\)</span> 通常初始化为零向量。 -
<strong>更新规则</strong>: <span class="math inline">\(s_t =
\sigma(W_{ss} s_{t-1} + W_{sx} x_t)\)</span> - 其中 <span
class="math inline">\(\sigma\)</span> 是激活函数，<span
class="math inline">\(W_{ss}\)</span> 和 <span
class="math inline">\(W_{sx}\)</span>
分别是状态到状态和输入到状态的权重矩阵。 - <strong>输出规则</strong>:
<span class="math inline">\(z_t = \Theta_{zs} s_t + \Theta_{zx}
x_t\)</span> - 其中 <span class="math inline">\(\Theta_{zs}\)</span> 和
<span class="math inline">\(\Theta_{zx}\)</span>
是从状态到输出的权重矩阵。 - <strong>成本</strong>: 每步计算的成本为
<span class="math inline">\(O(1)\)</span>。</p>
<h3 id="ttt-test-time-training">TTT (Test-Time Training)</h3>
<p><strong>数学表示</strong>: - <strong>初始状态</strong>: <span
class="math inline">\(W_0 = f.\text{params}()\)</span> - <span
class="math inline">\(W_0\)</span> 是模型参数的初始集合。 -
<strong>更新规则</strong>: <span class="math inline">\(W_t = W_{t-1} -
\eta \nabla \ell(W_{t-1}; x_t)\)</span> - <span
class="math inline">\(\eta\)</span> 是学习率，<span
class="math inline">\(\ell\)</span> 是自监督损失函数，<span
class="math inline">\(\nabla \ell\)</span> 是损失函数关于参数的梯度。 -
<strong>输出规则</strong>: <span class="math inline">\(z_t = f(x_t;
W_t)\)</span> - 输出是使用当前参数 <span
class="math inline">\(W_t\)</span> 和当前输入 <span
class="math inline">\(x_t\)</span> 通过模型函数 <span
class="math inline">\(f\)</span> 计算得到。 - <strong>成本</strong>:
每步更新的成本为 <span
class="math inline">\(O(1)\)</span>，与RNN相似，但TTT在每个时间步都进行模型参数的更新。</p>
<h3 id="主要区别">主要区别</h3>
<ol type="1">
<li><strong>状态表示</strong>:
<ul>
<li><strong>RNN</strong>: 使用隐状态 <span
class="math inline">\(s_t\)</span>
来储存过去信息，该状态通过固定的权重矩阵和当前输入更新。</li>
<li><strong>TTT</strong>: 使用模型的整体参数 <span
class="math inline">\(W_t\)</span>
作为状态，这些参数直接在每一步根据当前输入和损失函数更新。</li>
</ul></li>
<li><strong>更新机制</strong>:
<ul>
<li><strong>RNN</strong>:
状态更新依赖于前一状态和当前输入，通常是通过矩阵乘法和非线性激活函数完成。</li>
<li><strong>TTT</strong>:
参数更新通过梯度下降直接针对损失函数，这意味着TTT可以在测试时对模型进行微调，以适应可能的数据分布变化。</li>
</ul></li>
<li><strong>输出计算</strong>:
<ul>
<li><strong>RNN</strong>: 输出通常是当前状态的线性组合。</li>
<li><strong>TTT</strong>:
输出是使用当前时刻的模型参数对输入进行的直接评估。</li>
</ul></li>
<li><strong>灵活性和适应性</strong>:
<ul>
<li><strong>RNN</strong>:
结构比较固定，一旦训练完成，模型权重不再变化。</li>
<li><strong>TTT</strong>:
允许模型在使用过程中继续学习和适应，尤其适用于模型部署后环境可能发生变化的情况。</li>
</ul></li>
</ol>
<hr />
<h1
id="qa那么ttt和测试阶段使用train模式的rnn有什么区别">[QA]那么TTT和测试阶段使用Train模式的RNN有什么区别?</h1>
<p>TTT（Test Time
Training）和在测试时继续训练RNN（例如，将RNN设置为train模式进行测试）之间确实存在一些关键区别。这两种策略虽然都涉及到在模型部署后继续更新模型参数，但它们的目的、实施方式和适用场景有所不同。我将详细解释这些差异。</p>
<h2 id="目的和策略">目的和策略</h2>
<h3 id="继续训练rnn">继续训练RNN</h3>
<ul>
<li><strong>目的</strong>：通常用于微调或调整模型以适应新的或稍有不同的数据分布。这种策略可能是由于模型在实际应用中遇到了之前训练集中未充分表示的情况。</li>
<li><strong>策略</strong>：在测试阶段，保留反向传播和参数更新的能力。这意味着对于每个测试样本，模型不仅会进行预测，还可能根据预测结果和真实标签（如果可用）调整其权重。</li>
<li><strong>实施</strong>：这通常需要在测试时也提供真实标签，因此更类似于一个持续的训练过程而不是真正的测试。</li>
</ul>
<h3 id="ttt-1">TTT</h3>
<ul>
<li><strong>目的</strong>：设计TTT的初衷是为了使模型能够<strong>更好地泛化到在训练阶段未见过的新环境或条件</strong>，尤其是在<strong>数据分布可能发生变化</strong>的情况下。</li>
<li><strong>策略</strong>：TTT通常涉及在模型推理过程中使用一些未标记数据进行自适应调整。这可能包括使用<strong>伪标签或其他形式的自监着学习来调整模型参</strong>数。</li>
<li><strong>实施</strong>：TTT的一个关键组成部分是它通常<strong>不依赖于实际的标签数据进行更新</strong>，而是利用当前的输入数据和一个预先定义的自适应算法（例如，最小化输出的不确定性）来调整模型。</li>
</ul>
<h2 id="实施方式和适用场景">实施方式和适用场景</h2>
<h3 id="继续训练rnn-1">继续训练RNN</h3>
<ul>
<li><strong>适用场景</strong>：适用于有持续数据流且可以持续获得真实反馈的场景，如在线学习或增量学习场景。</li>
<li><strong>实施方式</strong>：需要对模型架构没有变化，但需要保证每次输入都能获得相应的标签以进行有效训练。</li>
</ul>
<h3 id="ttt-2">TTT</h3>
<ul>
<li><strong>适用场景</strong>：适用于模型部署后环境可能发生变化的情况，其中模型需要自我调整以适应新环境，而无需外部的标签反馈。</li>
<li><strong>实施方式</strong>：可能包括技术如自监督学习，使用生成的或伪造的标签来进行自我调整，不依赖于外部标签。</li>
</ul>
<hr />
<h1
id="ttt对sa的改进架构以及数学部分">TTT对SA的改进(架构以及数学部分)</h1>
<h2 id="数学的表达">数学的表达</h2>
<p>在这里，我们将探讨自注意力机制（Self-Attention,
SA）与测试时训练（Test-Time Training,
TTT）的区别。这两种技术都被用于处理序列数据，但它们的实现方法和目标有所不同。</p>
<h3 id="self-attention-sa">Self-Attention (SA)</h3>
<p><strong>数学表示</strong>: - <strong>初始状态</strong>: <span
class="math inline">\(s_0\)</span> 通常是一个列表，可以存储序列的历史。
- <strong>更新规则</strong>: <span class="math inline">\(s_t =
s_{t-1}.\text{append}(k_t, v_t)\)</span> - 这里 <span
class="math inline">\(k_t\)</span> 和 <span
class="math inline">\(v_t\)</span> 分别是时间步 <span
class="math inline">\(t\)</span> 的键和值，它们是从输入 <span
class="math inline">\(x_t\)</span> 计算得出。 -
<strong>输出规则</strong>: <span class="math inline">\(z_t = V_t
\text{softmax}(K_t^T q_t)\)</span> - <span
class="math inline">\(K_t\)</span> 是所有键的集合，<span
class="math inline">\(q_t\)</span> 是查询，<span
class="math inline">\(V_t\)</span>
是所有值的集合。输出是这些值的加权组合，权重由键和查询的相似度决定。 -
<strong>成本</strong>: 由于需要考虑所有前面的输入，计算复杂度与时间步
<span class="math inline">\(t\)</span> 成正比，即 <span
class="math inline">\(O(t)\)</span>。</p>
<h3 id="test-time-training-ttt">Test-Time Training (TTT)</h3>
<p><strong>数学表示</strong>: - <strong>初始状态</strong>: <span
class="math inline">\(W_0 = f.\text{params}()\)</span> - <span
class="math inline">\(W_0\)</span> 是模型参数的初始集合。 -
<strong>更新规则</strong>: <span class="math inline">\(W_t = W_{t-1} -
\eta \nabla \ell(W_{t-1}; x_t)\)</span> - <span
class="math inline">\(\eta\)</span> 是学习率，<span
class="math inline">\(\ell\)</span> 是自监督损失函数，<span
class="math inline">\(\nabla \ell\)</span> 是损失函数关于参数的梯度。 -
<strong>输出规则</strong>: <span class="math inline">\(z_t = f(x_t;
W_t)\)</span> - 输出是使用当前参数 <span
class="math inline">\(W_t\)</span> 和当前输入 <span
class="math inline">\(x_t\)</span> 通过模型函数 <span
class="math inline">\(f\)</span> 计算得到。 - <strong>成本</strong>:
每步更新的成本为 <span class="math inline">\(O(1)\)</span>。</p>
<h3 id="主要区别-1">主要区别</h3>
<ol type="1">
<li><strong>核心机制</strong>:
<ul>
<li><strong>Self-Attention</strong>:
通过计算输入元素之间的相互作用（通过键、查询和值）来捕捉序列内的长距离依赖关系。这种机制允许模型在每个时间步考虑到所有先前的输入。</li>
<li><strong>TTT</strong>:
在模型使用过程中继续通过梯度下降更新模型的参数，以适应新的数据或修正预测，增强了模型的适应性和灵活性。</li>
</ul></li>
<li><strong>更新策略</strong>:
<ul>
<li><strong>Self-Attention</strong>:
每个时间步的输出依赖于所有之前的输入，每次更新增加的计算复杂度随时间线性增长。</li>
<li><strong>TTT</strong>:
每个时间步对模型参数进行更新，但每步的计算复杂度保持不变，为 <span
class="math inline">\(O(1)\)</span>。</li>
</ul></li>
<li><strong>目标和应用</strong>:
<ul>
<li><strong>Self-Attention</strong>:
主要用于提高模型对序列数据内部结构的理解，尤其是在处理长序列时，能够有效捕获长范围依赖关系，广泛应用于自然语言处理和序列分析。</li>
<li><strong>TTT</strong>:
设计用于测试阶段，通过实时优化模型参数来适应新的或变化的数据分布，适合于动态环境中的应用，如在线学习或持续学习场景。</li>
</ul></li>
</ol>
<p>总结来说，Self-Attention
是一种强大的序列建模工具，能够捕获数据中的复杂依赖关系，而 TTT
提供了一种在实际应用中继续优化和调整模型的方法。这两种技术各有特点，适用于不同的场景和需求。</p>
<hr />
<h1
id="qa从头训练rnnsa和ttt有什么区别">[QA]从头训练,RNN,SA和TTT有什么区别?</h1>
<p>在论文中提到的TTT (Test-Time Training)
层与传统的RNN（循环神经网络）和SA（自注意力机制，如Transformer）在更新规则上的主要区别在于TTT层在训练时也采用了自监督学习的方式来更新隐藏状态。这种方法使得在测试时间（test
time）的表现可以通过训练的方式进行优化，即使在测试阶段也能继续进行模型的适应和学习。</p>
<h2 id="ttt层的更新规则">TTT层的更新规则</h2>
<p>TTT层的核心思想是将隐藏状态本身视为一个可训练的模型，而更新规则则是在自监督损失上进行梯度步骤的更新。这意味着，每一次输入一个新的测试序列时，隐藏状态（即模型）都会通过训练来进行更新和调整。具体来说，文档中提到的是，更新规则是一个关于自监督损失的梯度更新步骤，这与在训练时使用的方法相同。</p>
<h2 id="与传统rnn和sa的区别">与传统RNN和SA的区别</h2>
<h3 id="传统rnn"> <strong>传统RNN：</strong></h3>
<ul>
<li>传统RNN通常具有固定大小的隐藏状态，通过时间步递归地更新隐藏状态。</li>
<li>更新是基于前一个隐藏状态和当前输入，通常不包括在测试时更新模型的权重。</li>
</ul>
<h3 id="自注意力机制sa-1"><strong>自注意力机制（SA）：</strong></h3>
<ul>
<li>SA（如Transformer）通过对所有输入序列的全局注意力权重计算，不断更新其状态，处理长距离依赖问题效果较好。</li>
<li>它的复杂度是二次的，这在处理非常长的序列时会成为瓶颈。</li>
</ul>
<h3 id="ttt层"><strong>TTT层：</strong></h3>
<ul>
<li>TTT层通过将隐藏状态视为可训练的模型（如线性模型或MLP），在每个测试样本上进行更新，这种方式在测试时仍然可以进行学习和调整。</li>
<li>这种设计允许TTT层在处理长序列时保持线性复杂度，同时通过更加丰富的隐藏状态表达力来提高性能。</li>
</ul>
<hr />
<h1
id="qa那么也就是说训练ttt的时候必须要有验证集才能激活隐藏层自适应更新的能力吗">[QA]那么也就是说,训练TTT的时候,必须要有验证集,才能激活隐藏层自适应更新的能力吗?</h1>
<p>从文档内容来看，TTT（Test-Time
Training）层的关键特性是在测试时可以更新其隐藏状态。这种更新并不严格要求必须有与训练过程中相同的验证集，但为了实现有效的自适应和更新，需要至少有一些数据来进行这种测试时的“训练”。</p>
<h2 id="ttt层更新机制的理解">TTT层更新机制的理解</h2>
<p>TTT层的设计使其能在测试阶段对隐藏状态进行调整，这种调整是基于输入数据的自监督学习。自监督学习允许模型使用未标记的数据来调整自身，这意味着它可以在没有明确标签的情况下进行更新。因此，理论上TTT层可以在任何测试数据上进行自我更新，而不一定需要训练时使用的验证集。</p>
<h2 id="数据需求">数据需求</h2>
<p>尽管如此，为了使这种更新有效，这些测试数据应具备一定的质量和相关性，以确保更新步骤能够有效地改进模型性能，而不是导致性能退化。这就意味着，尽管不需要与训练过程中完全相同的验证集，但测试数据应该与模型训练时的数据在统计特性上有足够的相似性。</p>
<h3 id="实际应用考虑">实际应用考虑</h3>
<ul>
<li><strong>数据代表性</strong>：测试数据需要代表实际应用中模型将遇到的数据分布，以便模型可以适当地调整其参数。</li>
<li><strong>持续学习</strong>：在某些应用场景中，可能需要模型在接收到新数据时不断进行调整和优化，这种场景下TTT层特别有用。</li>
<li><strong>防止过拟合</strong>：在测试时对模型进行更新时，需要注意防止过拟合于特定的测试数据，特别是当测试数据量不大时。</li>
</ul>
<h1 id="更多请参考原论文-链接">更多请参考原论文-<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.04620">链接</a></h1>
<ol type="1">
<li><strong>具体的模型架构</strong>：原论文可能详细描述了使用的特定递归神经网络(RNN)的架构和变体。</li>
<li><strong>表达性隐藏状态的具体实现和优势</strong>：如何增强RNN的隐藏状态以提升模型的学习和泛化能力。</li>
<li><strong>测试时学习的实际应用和效果</strong>：原论文可能包含了在不同数据集上的实验结果，展示测试时学习对性能的实际影响。</li>
<li><strong>理论分析和数学建模</strong>：详细的理论分析可能用于解释为何在测试时进行学习能够提升模型表现。</li>
<li><strong>与其他方法的比较</strong>：对比其他类似技术或传统方法，原论文可能展示了本方法的独特优势和局限。</li>
<li></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button>
    Donate
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechat.png" alt="Jeffrey Jiang WeChat Pay">
        <span>WeChat Pay</span>
      </div>
      <div>
        <img src="/images/alipay.png" alt="Jeffrey Jiang Alipay">
        <span>Alipay</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Jeffrey Jiang
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://jefferyjiang-yf.github.io/2024/07/11/%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E6%8F%AD%E7%A7%98%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E9%9D%A9%E5%91%BD-%E6%96%B0%E6%A8%A1%E5%9E%8BTTT%E6%AD%A3%E9%9D%A9%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/" title="TTT人话版解读-Learning-to-Learn-at-Test-Time-RNNs-with-Expressive-Hidden-States">https://jefferyjiang-yf.github.io/2024/07/11/未来已来：揭秘下一代智能语言模型的自我学习革命-新模型TTT正革新大模型底层架构/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" rel="tag"><i class="fa fa-tag"></i> 自回归模型</a>
          </div>

        
  <div class="social-like a2a_kit a2a_kit_size_32 a2a_default_style">
    <a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a>
      <a class="a2a_button_facebook"></a>
      <a class="a2a_button_twitter"></a>
      <a class="a2a_button_wechat"></a>
      <a class="a2a_button_facebook_messenger"></a>
      <a class="a2a_button_email"></a>
      <a class="a2a_button_copy_link"></a>
  </div>

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/07/09/Normalization-techniques-in-Deep-Learning/" rel="prev" title="Normalization techniques in Deep Learning">
                  <i class="fa fa-angle-left"></i> Normalization techniques in Deep Learning
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/07/12/Comparasion-the-GPTs-with-scholar-target-used-in-CN-and-EN-prompt/" rel="next" title="Comparasion the GPTs with scholar-target used in CN and EN prompt">
                  Comparasion the GPTs with scholar-target used in CN and EN prompt <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments">
    <script src="https://giscus.app/client.js"
        data-repo="JefferyJiang-YF/JefferyJiang-YF.github.io" 
        data-repo-id="R_kgDOMPO3Yg" 
        data-category="Announcements"
        data-category-id="DIC_kwDOMPO3Ys4CgnPP"
        data-mapping="pathname" 
        data-reactions-enabled="1" 
        data-emit-metadata="1" 
        data-theme="dark"
        data-lang="en"
        crossorigin="anonymous"
        data-input-position="bottom"
        data-loading="lazy"
        async>
    </script>
  </div>
  
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Copyright ©  Jeffrey Jiang All Rights Reserved.</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">37k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">34 mins.</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/JefferyJiang-YF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.9.0/mermaid.min.js","integrity":"sha256-stuqcu2FrjYCXDOytWFA5SoUE/r3nkp6gTglzNSlavU="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script class="next-config" data-name="wavedrom" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.5.0/wavedrom.min.js","integrity":"sha256-INLAoJc6quTNfiMWkGZniYO2cxE8mHpddnLow1m6RFs="}}</script>
  <script class="next-config" data-name="wavedrom_skin" type="application/json">{"enable":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/wavedrom/3.5.0/skins/default.js","integrity":"sha256-fduc/Zszk5ezWws2uInY/ALWVmIrmV6VTgXbsYSReFI="}}</script>
  <script src="/js/third-party/tags/wavedrom.js"></script>


  <script src="/js/third-party/pace.js"></script>

  <script src="/js/third-party/addtoany.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://jefferyjiang-yf.github.io/2024/07/11/%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%EF%BC%9A%E6%8F%AD%E7%A7%98%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%87%AA%E6%88%91%E5%AD%A6%E4%B9%A0%E9%9D%A9%E5%91%BD-%E6%96%B0%E6%A8%A1%E5%9E%8BTTT%E6%AD%A3%E9%9D%A9%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
